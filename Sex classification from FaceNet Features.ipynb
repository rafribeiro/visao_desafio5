{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f15908",
   "metadata": {},
   "source": [
    "# Neural Network for classification of sex based on facial feature vectors\n",
    "Feature vectors obtained from FaceNet. Face detection with RetinaFace (insightface)\n",
    "\n",
    "Neural network with ad-hoc architecture (5 fully connected layers)\n",
    "\n",
    "Faces from Fairfaces train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2553f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76e90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a520c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading templates from gallery file.\n",
      "86744 templates loaded from gallery file.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading templates from gallery file.\")\n",
    "with open(\"/media/rafael/Windows-SSD/recfac/fairface_train_facenet.gal\",\"rb\") as f:\n",
    "    gallery = pickle.load(f, encoding=\"latin1\")\n",
    "print(\"{} templates loaded from gallery file.\".format(len(gallery)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa9d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86664, 86664)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = pd.read_csv('/media/rafael/Windows-SSD/recfac/bases/fairfaces/fairface_label_train.csv', header=0, index_col=0, usecols=[0,2] ).to_dict()['gender']\n",
    "labels_pd = pd.DataFrame.from_dict(labels_dict, orient='index', columns=['gender'])\n",
    "features_pd = pd.DataFrame.from_dict(gallery, orient='index', columns=['status','features'])\n",
    "bads = features_pd[features_pd['status'] == 'no face']\n",
    "keys = ['train/' + index for index in bads.index.tolist()]\n",
    "labels_pd.drop(index=keys, inplace=True)\n",
    "keys = [index for index in bads.index.tolist()]\n",
    "features_pd.drop(index=keys, inplace=True)\n",
    "len(features_pd), len(labels_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4de716",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pd.sort_index(axis=0, level=None, ascending=True, inplace=True, kind='quicksort')\n",
    "features_pd.sort_index(axis=0, level=None, ascending=True, inplace=True, kind='quicksort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e8a33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.jpg</th>\n",
       "      <td>ok</td>\n",
       "      <td>[0.22564129531383514, -0.5401293635368347, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.jpg</th>\n",
       "      <td>ok</td>\n",
       "      <td>[-0.7215465307235718, 0.26587170362472534, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.jpg</th>\n",
       "      <td>ok</td>\n",
       "      <td>[1.1927777528762817, 0.10555720329284668, 0.32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.jpg</th>\n",
       "      <td>ok</td>\n",
       "      <td>[0.8545346260070801, 1.7964006662368774, -2.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.jpg</th>\n",
       "      <td>ok</td>\n",
       "      <td>[-0.0444677472114563, -0.2601420283317566, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          status                                           features\n",
       "1.jpg         ok  [0.22564129531383514, -0.5401293635368347, -0....\n",
       "10.jpg        ok  [-0.7215465307235718, 0.26587170362472534, -0....\n",
       "100.jpg       ok  [1.1927777528762817, 0.10555720329284668, 0.32...\n",
       "1000.jpg      ok  [0.8545346260070801, 1.7964006662368774, -2.07...\n",
       "10000.jpg     ok  [-0.0444677472114563, -0.2601420283317566, -0...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831ecab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features_pd['features'])\n",
    "y = np.array(labels_pd['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca879a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack(list(zip(*X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265870f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y_enc =le.transform(y)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dfb254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float().to(device)\n",
    "y = torch.tensor(y_enc).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f780a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86664, 128), (86664,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(X.size()),tuple(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c7a0ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f83b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2256, -0.5401, -0.5190,  ..., -1.0052,  0.3861, -0.0374],\n",
       "        [-0.7215,  0.2659, -0.4371,  ..., -0.9571,  1.1012,  0.3123],\n",
       "        [ 1.1928,  0.1056,  0.3253,  ..., -0.0420, -0.6737, -1.5649],\n",
       "        ...,\n",
       "        [ 0.8388, -0.1194, -0.2304,  ..., -2.6394,  1.7664, -0.2935],\n",
       "        [-0.4859, -0.6224, -1.3966,  ..., -1.2480,  0.1008,  1.6479],\n",
       "        [-0.2739,  0.1696, -1.8366,  ..., -0.6585,  0.0236, -1.0443]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c6160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X.shape[1]\n",
    "C = 2\n",
    "H = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7b010c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lambda_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "456e00a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for t in range(epochs):\\n    \\n        # Feed forward to get the logits\\n        y_pred = model(X)\\n        \\n        # Compute the loss and accuracy\\n        loss = criterion(y_pred, y)\\n        score, predicted = torch.max(y_pred, 1)\\n        acc = (y == predicted).sum().float() / len(y)\\n        print(\"Training - [EPOCH]: {}, [LOSS]: {:.6f}, [ACCURACY]: {:.3f}\".format(t+1, loss.item(), acc),end=\\'\\r\\')\\n        #display.clear_output(wait=True)\\n        \\n        # zero the gradients before running\\n        # the backward pass.\\n        optimizer.zero_grad()\\n        \\n        # Backward pass to compute the gradient\\n        # of loss w.r.t our learnable params. \\n        loss.backward()\\n        \\n        # Update params\\n        optimizer.step()'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(X, y, n_epochs, batch_size):\n",
    "    global model\n",
    "    # nn package to create our linear model\n",
    "    # each Linear module has a weight and bias\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(D, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, C)\n",
    "    )\n",
    "    model.to(device)\n",
    "    # nn package also has different loss functions.\n",
    "    # we use cross entropy loss for our classification task\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # we use the optim package to apply\n",
    "    # ADAM for our parameter updates\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_l2) # built-in L2\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # X is a torch Variable\n",
    "        permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "        for i in range(0,X.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X[indices], y[indices]\n",
    "            \n",
    "            y_pred = model.forward(batch_x.to(device))\n",
    "            loss = criterion(y_pred,batch_y)\n",
    "            \n",
    "            score, predicted = torch.max(y_pred, 1)\n",
    "            acc = 100*(batch_y == predicted).sum().float() / len(batch_y)\n",
    "            \n",
    "            print(\"Epoch: {} | Batch: {}: | Loss: {:.4f} | Accuracy: {:.2f}\".format(epoch, i//batch_size+1, loss, acc),end='\\r')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print()\n",
    "    \n",
    "    \n",
    "\"\"\"for t in range(epochs):\n",
    "    \n",
    "        # Feed forward to get the logits\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Compute the loss and accuracy\n",
    "        loss = criterion(y_pred, y)\n",
    "        score, predicted = torch.max(y_pred, 1)\n",
    "        acc = (y == predicted).sum().float() / len(y)\n",
    "        print(\"Training - [EPOCH]: {}, [LOSS]: {:.6f}, [ACCURACY]: {:.3f}\".format(t+1, loss.item(), acc),end='\\r')\n",
    "        #display.clear_output(wait=True)\n",
    "        \n",
    "        # zero the gradients before running\n",
    "        # the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass to compute the gradient\n",
    "        # of loss w.r.t our learnable params. \n",
    "        loss.backward()\n",
    "        \n",
    "        # Update params\n",
    "        optimizer.step()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f4cbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, n):\n",
    "    acc=[]\n",
    "    err = []\n",
    "    print('K-fold cross Validation - {} folds.'.format(n))\n",
    "    for i in range(0,n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1./n)\n",
    "        train(X_train, y_train, 10, 1024)\n",
    "        y_pred = model(X_test.to(device))\n",
    "        score, predicted = torch.max(y_pred, 1) # predicted = class predicted, one-hot encoded\n",
    "        errors = (y_test != predicted).sum()\n",
    "        accuracy = 100*(1-float(errors)/len(y_test))\n",
    "        print(\"Validation - Errors - fold {}: {}/{}   -   Accuracy: {:.2f}%\".format(i+1,errors,len(y_test),accuracy))\n",
    "        acc.append(accuracy)\n",
    "        err.append(float(errors))\n",
    "    acc = np.array(acc)\n",
    "    err = np.array(err)\n",
    "    return acc, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b07dc96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold cross Validation - 5 folds.\n",
      "Epoch: 0 | Batch: 68: | Loss: 0.3000 | Accuracy: 86.58\n",
      "Epoch: 1 | Batch: 68: | Loss: 0.2750 | Accuracy: 86.45\n",
      "Epoch: 2 | Batch: 68: | Loss: 0.2631 | Accuracy: 88.24\n",
      "Epoch: 3 | Batch: 68: | Loss: 0.2853 | Accuracy: 86.72\n",
      "Epoch: 4 | Batch: 68: | Loss: 0.2463 | Accuracy: 88.38\n",
      "Epoch: 5 | Batch: 68: | Loss: 0.2169 | Accuracy: 91.70\n",
      "Epoch: 6 | Batch: 68: | Loss: 0.2439 | Accuracy: 89.49\n",
      "Epoch: 7 | Batch: 68: | Loss: 0.2354 | Accuracy: 88.38\n",
      "Epoch: 8 | Batch: 68: | Loss: 0.2542 | Accuracy: 88.52\n",
      "Epoch: 9 | Batch: 68: | Loss: 0.2153 | Accuracy: 89.90\n",
      "Validation - Errors - fold 1: 2068/17333   -   Accuracy: 88.07%\n",
      "Epoch: 0 | Batch: 68: | Loss: 0.2764 | Accuracy: 87.55\n",
      "Epoch: 1 | Batch: 68: | Loss: 0.2867 | Accuracy: 86.72\n",
      "Epoch: 2 | Batch: 68: | Loss: 0.2805 | Accuracy: 86.86\n",
      "Epoch: 3 | Batch: 68: | Loss: 0.2488 | Accuracy: 88.24\n",
      "Epoch: 4 | Batch: 68: | Loss: 0.2740 | Accuracy: 87.69\n",
      "Epoch: 5 | Batch: 68: | Loss: 0.2274 | Accuracy: 90.18\n",
      "Epoch: 6 | Batch: 68: | Loss: 0.2711 | Accuracy: 87.83\n",
      "Epoch: 7 | Batch: 68: | Loss: 0.2396 | Accuracy: 88.11\n",
      "Epoch: 8 | Batch: 68: | Loss: 0.1909 | Accuracy: 92.53\n",
      "Epoch: 9 | Batch: 68: | Loss: 0.2661 | Accuracy: 86.86\n",
      "Validation - Errors - fold 2: 2128/17333   -   Accuracy: 87.72%\n",
      "Epoch: 0 | Batch: 68: | Loss: 0.2965 | Accuracy: 84.79\n",
      "Epoch: 1 | Batch: 68: | Loss: 0.2742 | Accuracy: 88.11\n",
      "Epoch: 2 | Batch: 68: | Loss: 0.2866 | Accuracy: 88.11\n",
      "Epoch: 3 | Batch: 68: | Loss: 0.2490 | Accuracy: 88.52\n",
      "Epoch: 4 | Batch: 68: | Loss: 0.2692 | Accuracy: 87.55\n",
      "Epoch: 5 | Batch: 68: | Loss: 0.2479 | Accuracy: 90.46\n",
      "Epoch: 6 | Batch: 68: | Loss: 0.2152 | Accuracy: 90.32\n",
      "Epoch: 7 | Batch: 68: | Loss: 0.2250 | Accuracy: 90.59\n",
      "Epoch: 8 | Batch: 68: | Loss: 0.2339 | Accuracy: 89.07\n",
      "Epoch: 9 | Batch: 68: | Loss: 0.2449 | Accuracy: 88.80\n",
      "Validation - Errors - fold 3: 2076/17333   -   Accuracy: 88.02%\n",
      "Epoch: 0 | Batch: 68: | Loss: 0.2842 | Accuracy: 87.97\n",
      "Epoch: 1 | Batch: 68: | Loss: 0.2579 | Accuracy: 88.93\n",
      "Epoch: 2 | Batch: 68: | Loss: 0.2637 | Accuracy: 86.86\n",
      "Epoch: 3 | Batch: 68: | Loss: 0.2423 | Accuracy: 88.38\n",
      "Epoch: 4 | Batch: 68: | Loss: 0.2184 | Accuracy: 90.18\n",
      "Epoch: 5 | Batch: 68: | Loss: 0.2332 | Accuracy: 89.07\n",
      "Epoch: 6 | Batch: 68: | Loss: 0.2197 | Accuracy: 90.04\n",
      "Epoch: 7 | Batch: 68: | Loss: 0.2313 | Accuracy: 90.73\n",
      "Epoch: 8 | Batch: 68: | Loss: 0.2396 | Accuracy: 89.63\n",
      "Epoch: 9 | Batch: 68: | Loss: 0.2471 | Accuracy: 88.38\n",
      "Validation - Errors - fold 4: 2090/17333   -   Accuracy: 87.94%\n",
      "Epoch: 0 | Batch: 68: | Loss: 0.2970 | Accuracy: 86.72\n",
      "Epoch: 1 | Batch: 68: | Loss: 0.2614 | Accuracy: 87.83\n",
      "Epoch: 2 | Batch: 68: | Loss: 0.2457 | Accuracy: 88.38\n",
      "Epoch: 3 | Batch: 68: | Loss: 0.2367 | Accuracy: 88.93\n",
      "Epoch: 4 | Batch: 68: | Loss: 0.2371 | Accuracy: 89.21\n",
      "Epoch: 5 | Batch: 68: | Loss: 0.2621 | Accuracy: 87.55\n",
      "Epoch: 6 | Batch: 68: | Loss: 0.2615 | Accuracy: 87.83\n",
      "Epoch: 7 | Batch: 68: | Loss: 0.2220 | Accuracy: 89.63\n",
      "Epoch: 8 | Batch: 68: | Loss: 0.2287 | Accuracy: 88.93\n",
      "Epoch: 9 | Batch: 68: | Loss: 0.2666 | Accuracy: 87.14\n",
      "Validation - Errors - fold 5: 2161/17333   -   Accuracy: 87.53%\n",
      "Mean accuracy: 87.86%\n",
      "Mean errors/total: 2104.6/17332\n"
     ]
    }
   ],
   "source": [
    "n = 5 # number of folds\n",
    "accuracy, errors = cross_val(X,y,n)\n",
    "print('Mean accuracy: {:.2f}%\\nMean errors/total: {}/{}'.format(accuracy.mean(),errors.mean(),len(y)//n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "844a6441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 85: | Loss: 0.3042 | Accuracy: 85.03\n",
      "Epoch: 1 | Batch: 85: | Loss: 0.2642 | Accuracy: 87.04\n",
      "Epoch: 2 | Batch: 85: | Loss: 0.2509 | Accuracy: 88.12\n",
      "Epoch: 3 | Batch: 85: | Loss: 0.2604 | Accuracy: 86.11\n",
      "Epoch: 4 | Batch: 85: | Loss: 0.2791 | Accuracy: 87.04\n",
      "Epoch: 5 | Batch: 85: | Loss: 0.2264 | Accuracy: 89.20\n",
      "Epoch: 6 | Batch: 85: | Loss: 0.2270 | Accuracy: 89.35\n",
      "Epoch: 7 | Batch: 85: | Loss: 0.2037 | Accuracy: 90.90\n",
      "Epoch: 8 | Batch: 85: | Loss: 0.2203 | Accuracy: 90.90\n",
      "Epoch: 9 | Batch: 85: | Loss: 0.2291 | Accuracy: 89.66\n"
     ]
    }
   ],
   "source": [
    "train(X, y, 10, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b191f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/media/rafael/Windows-SSD/recfac/bases/fairfaces/sex_from_features_train_facenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3d453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
